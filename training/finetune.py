"""
å¾®è°ƒæ¨¡å— - åˆ†ç±»å™¨å¾®è°ƒè®­ç»ƒï¼ˆä¿®å¤ç‰ˆï¼‰
"""
import torch
import torch.nn as nn
import torch.optim as optim
import os
from config import Config


def create_finetune_optimizer(ijepa_model, classifier, stage, learning_rate):
    """åˆ›å»ºå¾®è°ƒä¼˜åŒ–å™¨ - ä¿®å¤ç‰ˆ
    
    Args:
        ijepa_model: I-JEPAæ¨¡å‹
        classifier: åˆ†ç±»å™¨
        stage: è®­ç»ƒé˜¶æ®µï¼ˆ1æˆ–2ï¼‰
        learning_rate: åŸºç¡€å­¦ä¹ ç‡
        
    Returns:
        optimizer: ä¼˜åŒ–å™¨
    """
    if stage == 1:
        # é˜¶æ®µ1ï¼šåªè®­ç»ƒåˆ†ç±»å™¨
        param_groups = [
            {'params': classifier.parameters(), 'lr': learning_rate}
        ]
    else:
        # é˜¶æ®µ2ï¼šå¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹çš„æœ€åå‡ å±‚ + åˆ†ç±»å™¨
        # éœ€è¦åŒºåˆ†ä¸åŒç±»å‹çš„æ¨¡å‹æ¶æ„
        
        # æ”¶é›†æ‰€æœ‰å‚æ•°IDï¼Œé¿å…é‡å¤
        param_ids = set()
        ijepa_params = []
        classifier_only_params = []
        
        # æ”¶é›†ijepa_modelçš„å¯è®­ç»ƒå‚æ•°
        for name, param in ijepa_model.named_parameters():
            if param.requires_grad:
                ijepa_params.append(param)
                param_ids.add(id(param))
        
        # æ”¶é›†classifierç‹¬æœ‰çš„å‚æ•°ï¼ˆæ’é™¤å·²ç»åœ¨ijepa_paramsä¸­çš„å‚æ•°ï¼‰
        for name, param in classifier.named_parameters():
            if param.requires_grad:
                if id(param) not in param_ids:
                    classifier_only_params.append(param)
                    param_ids.add(id(param))
        
        # åˆ›å»ºå‚æ•°ç»„
        param_groups = []
        
        # æ·»åŠ ijepa_modelçš„å‚æ•°ï¼ˆä½¿ç”¨è¾ƒå°çš„å­¦ä¹ ç‡ï¼‰
        if ijepa_params:
            param_groups.append({
                'params': ijepa_params, 
                'lr': learning_rate / Config.FINETUNE_LR_RATIO,
                'name': 'ijepa_params'  # æ·»åŠ åç§°ä»¥ä¾¿è°ƒè¯•
            })
        
        # æ·»åŠ classifierç‹¬æœ‰çš„å‚æ•°ï¼ˆä½¿ç”¨æ ‡å‡†å­¦ä¹ ç‡ï¼‰
        if classifier_only_params:
            param_groups.append({
                'params': classifier_only_params, 
                'lr': learning_rate,
                'name': 'classifier_params'  # æ·»åŠ åç§°ä»¥ä¾¿è°ƒè¯•
            })
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»»ä½•ç‹¬ç«‹å‚æ•°ï¼ˆå¯èƒ½classifierå®Œå…¨ä¾èµ–ijepaï¼‰
        # åˆ™ä½¿ç”¨æ‰€æœ‰classifierå‚æ•°
        if not param_groups:
            # è¿™ç§æƒ…å†µä¸‹ï¼Œå¯èƒ½æ˜¯åŸå§‹I-JEPAæ¶æ„
            param_groups = [
                {
                    'params': [p for n, p in ijepa_model.named_parameters() if p.requires_grad], 
                    'lr': learning_rate / Config.FINETUNE_LR_RATIO,
                    'name': 'ijepa_all'
                },
                {
                    'params': classifier.parameters(), 
                    'lr': learning_rate,
                    'name': 'classifier_all'
                }
            ]
            
            # å†æ¬¡æ£€æŸ¥æ˜¯å¦æœ‰é‡å¤å‚æ•°
            ijepa_param_ids = {id(p) for n, p in ijepa_model.named_parameters() if p.requires_grad}
            classifier_param_ids = {id(p) for p in classifier.parameters()}
            
            # å¦‚æœæœ‰é‡å¤ï¼Œåªä½¿ç”¨classifierå‚æ•°
            if ijepa_param_ids & classifier_param_ids:
                print("âš ï¸ æ£€æµ‹åˆ°å‚æ•°é‡å¤ï¼Œåªä½¿ç”¨classifierå‚æ•°")
                param_groups = [
                    {'params': classifier.parameters(), 'lr': learning_rate}
                ]
        
        # æ‰“å°å‚æ•°ç»„ä¿¡æ¯ä»¥ä¾¿è°ƒè¯•
        print(f"ğŸ“Š å‚æ•°ç»„ä¿¡æ¯:")
        for i, group in enumerate(param_groups):
            param_count = len(list(group['params']))
            group_name = group.get('name', f'group_{i}')
            print(f"  {group_name}: {param_count} ä¸ªå‚æ•°, lr={group['lr']:.2e}")
    
    optimizer = optim.AdamW(param_groups, weight_decay=Config.FINETUNE_WEIGHT_DECAY)
    return optimizer


def train_epoch(ijepa_model, classifier, train_loader, criterion, optimizer, device, epoch):
    """è®­ç»ƒä¸€ä¸ªepoch
    
    Args:
        ijepa_model: I-JEPAæ¨¡å‹
        classifier: åˆ†ç±»å™¨
        train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
        criterion: æŸå¤±å‡½æ•°
        optimizer: ä¼˜åŒ–å™¨
        device: è®¾å¤‡
        epoch: å½“å‰epoch
        
    Returns:
        avg_loss: å¹³å‡æŸå¤±
        accuracy: å‡†ç¡®ç‡
    """
    # æ ¹æ®epochå†³å®šæ˜¯å¦è®­ç»ƒI-JEPAæ¨¡å‹
    if epoch >= Config.FINETUNE_STAGE2_EPOCH:
        ijepa_model.train()
    else:
        ijepa_model.eval()
    
    classifier.train()
    
    total_loss = 0.0
    correct = 0
    total = 0
    
    for batch_idx, (data, target) in enumerate(train_loader):
        data, target = data.to(device), target.to(device)
        
        optimizer.zero_grad()
        
        # æå–ç‰¹å¾
        if epoch >= Config.FINETUNE_STAGE2_EPOCH:
            features = ijepa_model.encode(data)
        else:
            with torch.no_grad():
                features = ijepa_model.encode(data)
        
        # åˆ†ç±»
        output = classifier(features)
        loss = criterion(output, target)
        
        # åå‘ä¼ æ’­
        loss.backward()
        
        # æ¢¯åº¦è£å‰ª - éœ€è¦è·å–æ‰€æœ‰å‚ä¸è®­ç»ƒçš„å‚æ•°
        params_to_clip = []
        
        # æ”¶é›†æ‰€æœ‰éœ€è¦è£å‰ªçš„å‚æ•°
        if epoch >= Config.FINETUNE_STAGE2_EPOCH:
            # é˜¶æ®µ2ï¼šåŒ…æ‹¬ijepaå’Œclassifierçš„å‚æ•°
            for param_group in optimizer.param_groups:
                params_to_clip.extend(param_group['params'])
        else:
            # é˜¶æ®µ1ï¼šåªæœ‰classifierçš„å‚æ•°
            params_to_clip = list(classifier.parameters())
        
        # åº”ç”¨æ¢¯åº¦è£å‰ª
        if params_to_clip:
            torch.nn.utils.clip_grad_norm_(params_to_clip, max_norm=Config.GRADIENT_CLIP)
        
        optimizer.step()
        
        # ç»Ÿè®¡
        total_loss += loss.item()
        _, predicted = torch.max(output.data, 1)
        total += target.size(0)
        correct += (predicted == target).sum().item()
        
        # æ‰“å°è¿›åº¦
        if batch_idx % Config.LOG_INTERVAL == 0:
            print(f'Epoch {epoch+1}, Batch {batch_idx}/{len(train_loader)}, '
                  f'Loss: {loss.item():.4f}, Acc: {100 * correct / total:.2f}%')
    
    avg_loss = total_loss / len(train_loader)
    accuracy = 100 * correct / total
    
    return avg_loss, accuracy


def validate(ijepa_model, classifier, val_loader, criterion, device):
    """éªŒè¯æ¨¡å‹
    
    Args:
        ijepa_model: I-JEPAæ¨¡å‹
        classifier: åˆ†ç±»å™¨
        val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
        criterion: æŸå¤±å‡½æ•°
        device: è®¾å¤‡
        
    Returns:
        avg_loss: å¹³å‡æŸå¤±
        accuracy: å‡†ç¡®ç‡
    """
    ijepa_model.eval()
    classifier.eval()
    
    total_loss = 0.0
    correct = 0
    total = 0
    
    with torch.no_grad():
        for data, target in val_loader:
            data, target = data.to(device), target.to(device)
            
            # æå–ç‰¹å¾
            features = ijepa_model.encode(data)
            
            # åˆ†ç±»
            output = classifier(features)
            loss = criterion(output, target)
            
            # ç»Ÿè®¡
            total_loss += loss.item()
            _, predicted = torch.max(output.data, 1)
            total += target.size(0)
            correct += (predicted == target).sum().item()
    
    avg_loss = total_loss / len(val_loader)
    accuracy = 100 * correct / total
    
    return avg_loss, accuracy


def optimized_finetune_classifier(ijepa_model, classifier, train_loader, val_loader, 
                                 num_epochs=None, learning_rate=None):
    """ä¼˜åŒ–çš„åˆ†ç±»å™¨å¾®è°ƒï¼ˆä¿®å¤ç‰ˆï¼‰
    
    Args:
        ijepa_model: I-JEPAæ¨¡å‹
        classifier: åˆ†ç±»å™¨
        train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
        val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
        num_epochs: è®­ç»ƒè½®æ•°
        learning_rate: å­¦ä¹ ç‡
        
    Returns:
        classifier: è®­ç»ƒåçš„åˆ†ç±»å™¨
        history: è®­ç»ƒå†å²
    """
    # ä½¿ç”¨é»˜è®¤é…ç½®
    if num_epochs is None:
        num_epochs = Config.FINETUNE_EPOCHS
    if learning_rate is None:
        learning_rate = Config.FINETUNE_LR
    
    device = Config.DEVICE
    ijepa_model = ijepa_model.to(device)
    classifier = classifier.to(device)
    
    # æŸå¤±å‡½æ•°ï¼ˆå¸¦æ ‡ç­¾å¹³æ»‘ï¼‰
    criterion = nn.CrossEntropyLoss(label_smoothing=Config.LABEL_SMOOTHING)
    
    # åˆå§‹ä¼˜åŒ–å™¨ï¼ˆé˜¶æ®µ1ï¼‰
    optimizer = create_finetune_optimizer(ijepa_model, classifier, stage=1, learning_rate=learning_rate)
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler = optim.lr_scheduler.OneCycleLR(
        optimizer, max_lr=learning_rate, 
        epochs=num_epochs, steps_per_epoch=len(train_loader),
        pct_start=0.1, anneal_strategy='cos'
    )
    
    # è®­ç»ƒå†å²
    history = {
        'train_loss': [],
        'val_loss': [],
        'train_acc': [],
        'val_acc': []
    }
    
    best_val_acc = 0.0
    patience_counter = 0
    
    print("ğŸš€ å¼€å§‹ä¼˜åŒ–ç‰ˆåˆ†ç±»å™¨å¾®è°ƒ...")
    
    for epoch in range(num_epochs):
        # é˜¶æ®µåˆ‡æ¢
        if epoch == Config.FINETUNE_STAGE2_EPOCH:
            print(f"ğŸ”„ åˆ‡æ¢åˆ°é˜¶æ®µ2ï¼šå¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹çš„æœ€åå‡ å±‚")
            
            # è§£å†»æœ€åå‡ å±‚
            for name, param in ijepa_model.named_parameters():
                # æ ¹æ®æ¨¡å‹ç±»å‹å†³å®šè§£å†»ç­–ç•¥
                if hasattr(ijepa_model, 'context_encoder'):
                    # åŸå§‹I-JEPAæˆ–HybridIJEPA
                    if 'blocks.10' in name or 'blocks.11' in name or 'feature_enhancer' in name:
                        param.requires_grad = True
                        print(f"  è§£å†»å±‚: {name}")
                else:
                    # å…¶ä»–æ¨¡å‹ç±»å‹
                    if 'layer4' in name or 'head' in name:
                        param.requires_grad = True
                        print(f"  è§£å†»å±‚: {name}")
            
            # é‡æ–°åˆ›å»ºä¼˜åŒ–å™¨ï¼ˆä¿®å¤ç‰ˆï¼‰
            optimizer = create_finetune_optimizer(
                ijepa_model, classifier, stage=2, learning_rate=learning_rate
            )
            
            # é‡æ–°åˆ›å»ºè°ƒåº¦å™¨
            remaining_epochs = num_epochs - Config.FINETUNE_STAGE2_EPOCH
            
            # è·å–æ‰€æœ‰å­¦ä¹ ç‡
            max_lrs = []
            for group in optimizer.param_groups:
                max_lrs.append(group['lr'])
            
            scheduler = optim.lr_scheduler.OneCycleLR(
                optimizer, 
                max_lr=max_lrs if len(max_lrs) > 1 else max_lrs[0], 
                epochs=remaining_epochs, 
                steps_per_epoch=len(train_loader)
            )
        
        # è®­ç»ƒ
        train_loss, train_acc = train_epoch(
            ijepa_model, classifier, train_loader, 
            criterion, optimizer, device, epoch
        )
        
        # æ›´æ–°å­¦ä¹ ç‡
        scheduler.step()
        
        # éªŒè¯
        val_loss, val_acc = validate(
            ijepa_model, classifier, val_loader, criterion, device
        )
        
        # è®°å½•å†å²
        history['train_loss'].append(train_loss)
        history['val_loss'].append(val_loss)
        history['train_acc'].append(train_acc)
        history['val_acc'].append(val_acc)
        
        # æ‰“å°ç»“æœ
        print(f'ğŸ“Š Epoch {epoch+1}/{num_epochs}:')
        print(f'  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')
        print(f'  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')
        print('-' * 60)
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            patience_counter = 0
            
            save_finetune_checkpoint(
                ijepa_model, classifier, epoch, 
                val_acc, val_loss, train_acc, train_loss
            )
            print(f"âœ… ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ŒéªŒè¯å‡†ç¡®ç‡: {val_acc:.2f}%")
        else:
            patience_counter += 1
        
        # æ—©åœ
        if patience_counter >= Config.PATIENCE_FINETUNE:
            print(f"æ—©åœè§¦å‘ï¼Œåœ¨ç¬¬ {epoch+1} è½®åœæ­¢è®­ç»ƒ")
            break
    
    # åŠ è½½æœ€ä½³æ¨¡å‹
    ijepa_model, classifier = load_finetune_checkpoint(ijepa_model, classifier)
    
    print(f"ğŸ‰ ä¼˜åŒ–ç‰ˆå¾®è°ƒå®Œæˆ! æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.2f}%")
    
    return classifier, history


def save_finetune_checkpoint(ijepa_model, classifier, epoch, val_acc, val_loss, train_acc, train_loss):
    """ä¿å­˜å¾®è°ƒæ£€æŸ¥ç‚¹
    
    Args:
        ijepa_model: I-JEPAæ¨¡å‹
        classifier: åˆ†ç±»å™¨
        epoch: å½“å‰epoch
        val_acc: éªŒè¯å‡†ç¡®ç‡
        val_loss: éªŒè¯æŸå¤±
        train_acc: è®­ç»ƒå‡†ç¡®ç‡
        train_loss: è®­ç»ƒæŸå¤±
    """
    checkpoint = {
        'epoch': epoch + 1,
        'ijepa_state': ijepa_model.state_dict(),
        'classifier_state': classifier.state_dict(),
        'val_accuracy': val_acc,
        'val_loss': val_loss,
        'train_accuracy': train_acc,
        'train_loss': train_loss
    }
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs(Config.CHECKPOINT_DIR, exist_ok=True)
    
    # ä¿å­˜æ£€æŸ¥ç‚¹
    checkpoint_path = os.path.join(Config.CHECKPOINT_DIR, Config.FINETUNE_MODEL_PATH)
    torch.save(checkpoint, checkpoint_path)


def load_finetune_checkpoint(ijepa_model, classifier, checkpoint_path=None):
    """åŠ è½½å¾®è°ƒæ£€æŸ¥ç‚¹
    
    Args:
        ijepa_model: I-JEPAæ¨¡å‹
        classifier: åˆ†ç±»å™¨
        checkpoint_path: æ£€æŸ¥ç‚¹è·¯å¾„
        
    Returns:
        ijepa_model: åŠ è½½äº†æƒé‡çš„I-JEPAæ¨¡å‹
        classifier: åŠ è½½äº†æƒé‡çš„åˆ†ç±»å™¨
    """
    if checkpoint_path is None:
        checkpoint_path = os.path.join(Config.CHECKPOINT_DIR, Config.FINETUNE_MODEL_PATH)
    
    try:
        checkpoint = torch.load(checkpoint_path, map_location=Config.DEVICE, weights_only=False)
        ijepa_model.load_state_dict(checkpoint['ijepa_state'])
        classifier.load_state_dict(checkpoint['classifier_state'])
        print(f"âœ… åŠ è½½æœ€ä½³æ¨¡å‹ï¼ŒéªŒè¯å‡†ç¡®ç‡: {checkpoint['val_accuracy']:.2f}%")
        return ijepa_model, classifier
    except Exception as e:
        print(f"âŒ åŠ è½½å¾®è°ƒæ¨¡å‹å¤±è´¥: {e}")
        return ijepa_model, classifier