"""
é¢„è®­ç»ƒæ¨¡å— - I-JEPAè‡ªç›‘ç£é¢„è®­ç»ƒ
"""
import torch
import torch.optim as optim
import numpy as np
import time
import math
import os
from config import Config
from models.encoders import update_target_encoder
from data.dataset import create_optimized_context_and_target_patches
from utils.losses import compute_enhanced_loss


def create_optimizer(model, learning_rate, weight_decay, betas):
    """åˆ›å»ºä¼˜åŒ–å™¨
    
    Args:
        model: æ¨¡å‹
        learning_rate: å­¦ä¹ ç‡
        weight_decay: æƒé‡è¡°å‡
        betas: Adamçš„betaå‚æ•°
        
    Returns:
        optimizer: ä¼˜åŒ–å™¨
    """
    # åˆ†ç¦»éœ€è¦å’Œä¸éœ€è¦æƒé‡è¡°å‡çš„å‚æ•°
    param_groups = [
        {
            'params': [p for n, p in model.named_parameters() 
                      if 'bias' not in n and p.requires_grad], 
            'weight_decay': weight_decay
        },
        {
            'params': [p for n, p in model.named_parameters() 
                      if 'bias' in n and p.requires_grad], 
            'weight_decay': 0.0
        }
    ]
    
    optimizer = optim.AdamW(param_groups, lr=learning_rate, betas=betas)
    return optimizer


def create_scheduler(optimizer, num_epochs, warmup_epochs):
    """åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨
    
    Args:
        optimizer: ä¼˜åŒ–å™¨
        num_epochs: æ€»è®­ç»ƒè½®æ•°
        warmup_epochs: é¢„çƒ­è½®æ•°
        
    Returns:
        scheduler: å­¦ä¹ ç‡è°ƒåº¦å™¨
    """
    def lr_lambda(epoch):
        if epoch < warmup_epochs:
            return epoch / warmup_epochs
        else:
            return 0.5 * (1 + math.cos(math.pi * (epoch - warmup_epochs) / (num_epochs - warmup_epochs)))
    
    scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)
    return scheduler


def train_one_epoch(model, train_loader, optimizer, device, epoch, log_interval=100):
    """è®­ç»ƒä¸€ä¸ªepoch
    
    Args:
        model: I-JEPAæ¨¡å‹
        train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
        optimizer: ä¼˜åŒ–å™¨
        device: è®¾å¤‡
        epoch: å½“å‰epoch
        log_interval: æ—¥å¿—æ‰“å°é—´éš”
        
    Returns:
        avg_loss: å¹³å‡æŸå¤±
        avg_cosine_sim: å¹³å‡ä½™å¼¦ç›¸ä¼¼åº¦
        avg_feature_std: å¹³å‡ç‰¹å¾æ ‡å‡†å·®
    """
    model.train()
    epoch_loss = 0.0
    cosine_similarities = []
    feature_stds = []
    
    for batch_idx, images in enumerate(train_loader):
        images = images.to(device)
        batch_size = images.size(0)
        
        # ç”Ÿæˆæ©ç ç­–ç•¥
        context_patches, target_patches = create_optimized_context_and_target_patches(
            batch_size, model.n_patches, mask_ratio=Config.MASK_RATIO
        )
        
        optimizer.zero_grad()
        
        try:
            # å‰å‘ä¼ æ’­
            predictions, targets = model(images, context_patches, target_patches)
            
            # è®¡ç®—æŸå¤±
            loss, cosine_sim = compute_enhanced_loss(predictions, targets)
            
            # åå‘ä¼ æ’­
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=Config.GRADIENT_CLIP)
            
            # ä¼˜åŒ–å™¨æ­¥éª¤
            optimizer.step()
            
            # æ›´æ–°ç›®æ ‡ç¼–ç å™¨ï¼ˆEMAï¼‰
            momentum = Config.EMA_MOMENTUM + (Config.EMA_MOMENTUM_END - Config.EMA_MOMENTUM) * epoch / Config.PRETRAIN_EPOCHS
            update_target_encoder(model.context_encoder, model.target_encoder, momentum=momentum)
            
            # è®°å½•æŒ‡æ ‡
            epoch_loss += loss.item()
            cosine_similarities.append(cosine_sim.item())
            
            # è®¡ç®—ç‰¹å¾å¤šæ ·æ€§
            import torch.nn.functional as F
            pred_std = torch.std(F.normalize(predictions, dim=-1), dim=0).mean()
            feature_stds.append(pred_std.item())
            
            # æ‰“å°æ—¥å¿—
            if batch_idx % log_interval == 0:
                print(f'Epoch {epoch+1}, Batch {batch_idx}/{len(train_loader)}, '
                      f'Loss: {loss.item():.4f}, Cosine Sim: {cosine_sim.item():.4f}, '
                      f'Feature Std: {pred_std.item():.4f}')
                
        except Exception as e:
            print(f"è®­ç»ƒæ­¥éª¤å‡ºé”™: {e}")
            continue
    
    # è®¡ç®—å¹³å‡å€¼
    avg_loss = epoch_loss / len(train_loader) if len(train_loader) > 0 else 0
    avg_cosine_sim = np.mean(cosine_similarities) if cosine_similarities else 0
    avg_feature_std = np.mean(feature_stds) if feature_stds else 0
    
    return avg_loss, avg_cosine_sim, avg_feature_std


def optimized_pretrain_ijepa(model, train_loader, num_epochs=None, learning_rate=None):
    """ä¼˜åŒ–çš„I-JEPAé¢„è®­ç»ƒ
    
    Args:
        model: I-JEPAæ¨¡å‹
        train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
        num_epochs: è®­ç»ƒè½®æ•°ï¼ˆé»˜è®¤ä½¿ç”¨é…ç½®ï¼‰
        learning_rate: å­¦ä¹ ç‡ï¼ˆé»˜è®¤ä½¿ç”¨é…ç½®ï¼‰
        
    Returns:
        model: è®­ç»ƒåçš„æ¨¡å‹
        history: è®­ç»ƒå†å²
    """
    # ä½¿ç”¨é…ç½®ä¸­çš„é»˜è®¤å€¼
    if num_epochs is None:
        num_epochs = Config.PRETRAIN_EPOCHS
    if learning_rate is None:
        learning_rate = Config.PRETRAIN_LR
        
    device = Config.DEVICE
    model = model.to(device)
    
    # åˆ›å»ºä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
    optimizer = create_optimizer(
        model, learning_rate, 
        Config.PRETRAIN_WEIGHT_DECAY, 
        Config.PRETRAIN_BETAS
    )
    scheduler = create_scheduler(optimizer, num_epochs, Config.WARMUP_EPOCHS)
    
    print("ğŸš€ å¼€å§‹ä¼˜åŒ–ç‰ˆI-JEPAé¢„è®­ç»ƒ...")
    
    # è®­ç»ƒå†å²
    history = {
        'loss': [],
        'cosine_similarity': [],
        'feature_diversity': [],
        'learning_rate': []
    }
    
    best_loss = float('inf')
    best_cosine_sim = 0.0
    patience = 0
    
    start_time = time.time()
    
    for epoch in range(num_epochs):
        # è®­ç»ƒä¸€ä¸ªepoch
        avg_loss, avg_cosine_sim, avg_feature_std = train_one_epoch(
            model, train_loader, optimizer, device, epoch, Config.LOG_INTERVAL
        )
        
        # æ›´æ–°å­¦ä¹ ç‡
        scheduler.step()
        current_lr = optimizer.param_groups[0]['lr']
        
        # è®°å½•å†å²
        history['loss'].append(avg_loss)
        history['cosine_similarity'].append(avg_cosine_sim)
        history['feature_diversity'].append(avg_feature_std)
        history['learning_rate'].append(current_lr)
        
        # æ‰“å°epochæ€»ç»“
        print(f'ğŸ“Š Epoch {epoch+1}/{num_epochs}:')
        print(f'  å¹³å‡æŸå¤±: {avg_loss:.4f}')
        print(f'  ä½™å¼¦ç›¸ä¼¼åº¦: {avg_cosine_sim:.4f}')
        print(f'  ç‰¹å¾å¤šæ ·æ€§: {avg_feature_std:.4f}')
        print(f'  å­¦ä¹ ç‡: {current_lr:.2e}')
        print('-' * 60)
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if avg_loss < best_loss:
            best_loss = avg_loss
            best_cosine_sim = avg_cosine_sim
            patience = 0
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            save_pretrain_checkpoint(
                model, optimizer, epoch, avg_loss, 
                avg_cosine_sim, avg_feature_std, time.time() - start_time
            )
            print(f"âœ… ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ŒæŸå¤±: {avg_loss:.4f}, ä½™å¼¦ç›¸ä¼¼åº¦: {avg_cosine_sim:.4f}")
        else:
            patience += 1
            
        # æ—©åœ
        if patience >= Config.PATIENCE_PRETRAIN:
            print(f"æ—©åœè§¦å‘ï¼Œåœ¨ç¬¬ {epoch+1} è½®åœæ­¢è®­ç»ƒ")
            break
    
    total_training_time = time.time() - start_time
    
    # åŠ è½½æœ€ä½³æ¨¡å‹
    model = load_pretrain_checkpoint(model)
    
    print(f"ğŸ‰ ä¼˜åŒ–ç‰ˆI-JEPAé¢„è®­ç»ƒå®Œæˆ! æ€»è€—æ—¶: {total_training_time:.1f}ç§’")
    
    return model, history


def save_pretrain_checkpoint(model, optimizer, epoch, loss, cosine_sim, feature_std, training_time):
    """ä¿å­˜é¢„è®­ç»ƒæ£€æŸ¥ç‚¹
    
    Args:
        model: æ¨¡å‹
        optimizer: ä¼˜åŒ–å™¨
        epoch: å½“å‰epoch
        loss: æŸå¤±å€¼
        cosine_sim: ä½™å¼¦ç›¸ä¼¼åº¦
        feature_std: ç‰¹å¾æ ‡å‡†å·®
        training_time: è®­ç»ƒæ—¶é—´
    """
    checkpoint = {
        'epoch': epoch + 1,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'loss': loss,
        'cosine_similarity': cosine_sim,
        'feature_diversity': feature_std,
        'training_time': training_time
    }
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs(Config.CHECKPOINT_DIR, exist_ok=True)
    
    # ä¿å­˜æ£€æŸ¥ç‚¹
    checkpoint_path = os.path.join(Config.CHECKPOINT_DIR, Config.PRETRAIN_MODEL_PATH)
    torch.save(checkpoint, checkpoint_path)


def load_pretrain_checkpoint(model, checkpoint_path=None):
    """åŠ è½½é¢„è®­ç»ƒæ£€æŸ¥ç‚¹
    
    Args:
        model: æ¨¡å‹
        checkpoint_path: æ£€æŸ¥ç‚¹è·¯å¾„ï¼ˆé»˜è®¤ä½¿ç”¨é…ç½®ï¼‰
        
    Returns:
        model: åŠ è½½äº†æƒé‡çš„æ¨¡å‹
    """
    if checkpoint_path is None:
        checkpoint_path = os.path.join(Config.CHECKPOINT_DIR, Config.PRETRAIN_MODEL_PATH)
    
    try:
        checkpoint = torch.load(checkpoint_path, map_location=Config.DEVICE)
        model.load_state_dict(checkpoint['model_state_dict'])
        print(f"âœ… åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ï¼ŒæŸå¤±: {checkpoint['loss']:.4f}")
        return model
    except Exception as e:
        print(f"âŒ åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å¤±è´¥: {e}")
        return model