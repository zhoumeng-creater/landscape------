"""
æ•°æ®é›†å’Œæ•°æ®åŠ è½½æ¨¡å—
"""
import os
import random
import numpy as np
from collections import Counter
from PIL import Image
import torch
from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder


class OptimizedGardenDataset(Dataset):
    """ä¼˜åŒ–çš„å›­æ—æ•°æ®é›†ç±»
    
    æ”¯æŒé¢„è®­ç»ƒå’Œå¾®è°ƒä¸¤ç§æ¨¡å¼
    """
    def __init__(self, image_paths, labels=None, transform=None, is_pretraining=False):
        """
        Args:
            image_paths: å›¾åƒè·¯å¾„åˆ—è¡¨
            labels: æ ‡ç­¾åˆ—è¡¨ï¼ˆé¢„è®­ç»ƒæ—¶ä¸ºNoneï¼‰
            transform: æ•°æ®å˜æ¢
            is_pretraining: æ˜¯å¦æ˜¯é¢„è®­ç»ƒæ¨¡å¼
        """
        self.image_paths = image_paths
        self.labels = labels
        self.transform = transform
        self.is_pretraining = is_pretraining
    
    def __len__(self):
        return len(self.image_paths)
    
    def __getitem__(self, idx):
        try:
            # åŠ è½½å›¾åƒ
            image = Image.open(self.image_paths[idx]).convert('RGB')
            
            # åº”ç”¨æ•°æ®å˜æ¢
            if self.transform:
                image = self.transform(image)
            
            # æ ¹æ®æ¨¡å¼è¿”å›ä¸åŒçš„æ•°æ®
            if self.is_pretraining:
                return image
            else:
                label = self.labels[idx]
                return image, label
                
        except Exception as e:
            print(f"Error loading image {self.image_paths[idx]}: {e}")
            # è¿”å›ä¸€ä¸ªé»˜è®¤å›¾åƒ
            default_image = Image.new('RGB', (224, 224), (255, 255, 255))
            if self.transform:
                default_image = self.transform(default_image)
            
            if self.is_pretraining:
                return default_image
            else:
                return default_image, self.labels[idx] if self.labels else 0


def load_dataset(data_path):
    """åŠ è½½æ•°æ®é›†
    
    Args:
        data_path: æ•°æ®é›†æ ¹ç›®å½•è·¯å¾„
        
    Returns:
        image_paths: å›¾åƒè·¯å¾„åˆ—è¡¨
        labels: æ ‡ç­¾åˆ—è¡¨
        class_names: ç±»åˆ«åç§°åˆ—è¡¨
    """
    image_paths = []
    labels = []
    class_names = []
    
    print("ğŸ“‚ æ­£åœ¨åŠ è½½æ•°æ®é›†...")
    
    # æ”¯æŒçš„å›¾åƒæ ¼å¼
    valid_extensions = ('.jpg', '.jpeg', '.png', '.bmp', '.gif')
    
    # éå†æ•°æ®ç›®å½•
    for class_folder in sorted(os.listdir(data_path)):
        class_path = os.path.join(data_path, class_folder)
        if os.path.isdir(class_path):
            class_names.append(class_folder)
            
            # éå†ç±»åˆ«æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰å›¾åƒ
            for root, dirs, files in os.walk(class_path):
                for file in files:
                    if file.lower().endswith(valid_extensions):
                        image_path = os.path.join(root, file)
                        image_paths.append(image_path)
                        labels.append(class_folder)
    
    print(f"âœ… æ‰¾åˆ° {len(class_names)} ä¸ªç±»åˆ«")
    print(f"âœ… æ€»å…± {len(image_paths)} å¼ å›¾åƒ")
    
    # æ‰“å°ç±»åˆ«åˆ†å¸ƒ
    label_counts = Counter(labels)
    print("\nğŸ“Š ç±»åˆ«åˆ†å¸ƒ:")
    for class_name, count in sorted(label_counts.items()):
        print(f"  {class_name}: {count} å¼ å›¾åƒ")
    
    return image_paths, labels, class_names


def split_dataset(image_paths, labels, test_size=0.2, val_size=0.1, random_state=42):
    """åˆ†å‰²æ•°æ®é›†ä¸ºè®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†
    
    Args:
        image_paths: å›¾åƒè·¯å¾„åˆ—è¡¨
        labels: æ ‡ç­¾åˆ—è¡¨
        test_size: æµ‹è¯•é›†æ¯”ä¾‹
        val_size: éªŒè¯é›†æ¯”ä¾‹ï¼ˆç›¸å¯¹äºæ€»æ•°æ®ï¼‰
        random_state: éšæœºç§å­
        
    Returns:
        (X_train, y_train): è®­ç»ƒé›†
        (X_val, y_val): éªŒè¯é›†
        (X_test, y_test): æµ‹è¯•é›†
        label_encoder: æ ‡ç­¾ç¼–ç å™¨
    """
    # æ ‡ç­¾ç¼–ç 
    label_encoder = LabelEncoder()
    encoded_labels = label_encoder.fit_transform(labels)
    
    # å…ˆåˆ†å‡ºæµ‹è¯•é›†
    X_temp, X_test, y_temp, y_test = train_test_split(
        image_paths, encoded_labels, 
        test_size=test_size, 
        stratify=encoded_labels, 
        random_state=random_state
    )
    
    # å†ä»å‰©ä½™æ•°æ®ä¸­åˆ†å‡ºéªŒè¯é›†
    val_size_adjusted = val_size / (1 - test_size)
    X_train, X_val, y_train, y_val = train_test_split(
        X_temp, y_temp, 
        test_size=val_size_adjusted, 
        stratify=y_temp, 
        random_state=random_state
    )
    
    print(f"\nğŸ“Š æ•°æ®é›†åˆ†å‰²å®Œæˆ:")
    print(f"  è®­ç»ƒé›†: {len(X_train)} å¼ å›¾åƒ")
    print(f"  éªŒè¯é›†: {len(X_val)} å¼ å›¾åƒ")  
    print(f"  æµ‹è¯•é›†: {len(X_test)} å¼ å›¾åƒ")
    
    # æ£€æŸ¥ç±»åˆ«å¹³è¡¡
    print("\nğŸ“ˆ å„æ•°æ®é›†ç±»åˆ«åˆ†å¸ƒ:")
    for name, y in [("è®­ç»ƒé›†", y_train), ("éªŒè¯é›†", y_val), ("æµ‹è¯•é›†", y_test)]:
        unique, counts = np.unique(y, return_counts=True)
        print(f"\n{name}:")
        for cls_idx, count in zip(unique, counts):
            cls_name = label_encoder.inverse_transform([cls_idx])[0]
            print(f"  {cls_name}: {count} å¼ ")
    
    return (X_train, y_train), (X_val, y_val), (X_test, y_test), label_encoder


def create_balanced_dataloader(dataset, labels, batch_size=32, num_workers=2):
    """åˆ›å»ºå¹³è¡¡çš„æ•°æ®åŠ è½½å™¨
    
    ä½¿ç”¨åŠ æƒéšæœºé‡‡æ ·æ¥å¹³è¡¡ç±»åˆ«
    
    Args:
        dataset: æ•°æ®é›†å®ä¾‹
        labels: æ ‡ç­¾åˆ—è¡¨
        batch_size: æ‰¹æ¬¡å¤§å°
        num_workers: å·¥ä½œè¿›ç¨‹æ•°
        
    Returns:
        dataloader: å¹³è¡¡çš„æ•°æ®åŠ è½½å™¨
    """
    # è®¡ç®—ç±»åˆ«æƒé‡
    class_counts = Counter(labels)
    total_samples = len(labels)
    class_weights = {cls: total_samples / count for cls, count in class_counts.items()}
    
    # åˆ›å»ºæ ·æœ¬æƒé‡
    sample_weights = [class_weights[label] for label in labels]
    
    # åˆ›å»ºåŠ æƒé‡‡æ ·å™¨
    sampler = WeightedRandomSampler(
        sample_weights, 
        len(sample_weights), 
        replacement=True
    )
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    dataloader = DataLoader(
        dataset, 
        batch_size=batch_size, 
        sampler=sampler, 
        num_workers=num_workers,
        pin_memory=True
    )
    
    return dataloader


def create_dataloader(dataset, batch_size=32, shuffle=True, num_workers=2):
    """åˆ›å»ºæ ‡å‡†æ•°æ®åŠ è½½å™¨
    
    Args:
        dataset: æ•°æ®é›†å®ä¾‹
        batch_size: æ‰¹æ¬¡å¤§å°
        shuffle: æ˜¯å¦æ‰“ä¹±
        num_workers: å·¥ä½œè¿›ç¨‹æ•°
        
    Returns:
        dataloader: æ•°æ®åŠ è½½å™¨
    """
    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=shuffle,
        num_workers=num_workers,
        pin_memory=True,
        drop_last=False
    )
    
    return dataloader


def create_optimized_context_and_target_patches(batch_size, n_patches, mask_ratio=0.75):
    """ä¼˜åŒ–çš„æ©ç ç­–ç•¥ - ä½¿ç”¨å—çŠ¶æ©ç 
    
    ç”Ÿæˆç”¨äºI-JEPAé¢„è®­ç»ƒçš„ä¸Šä¸‹æ–‡å’Œç›®æ ‡patches
    
    Args:
        batch_size: æ‰¹æ¬¡å¤§å°
        n_patches: æ€»patchæ•°é‡
        mask_ratio: æ©ç æ¯”ä¾‹
        
    Returns:
        context_patches_list: æ¯ä¸ªæ ·æœ¬çš„å¯è§patchç´¢å¼•
        target_patches_list: æ¯ä¸ªæ ·æœ¬çš„ç›®æ ‡patchç´¢å¼•
    """
    context_patches_list = []
    target_patches_list = []
    
    mask_count = int(n_patches * mask_ratio)
    grid_size = int(n_patches ** 0.5)
    
    for _ in range(batch_size):
        # åˆ›å»ºè¿ç»­çš„æ©ç å—
        masked_patches = set()
        
        # é€‰æ‹©2-4ä¸ªæ©ç ä¸­å¿ƒ
        num_centers = random.randint(2, 4)
        centers = random.sample(range(n_patches), min(num_centers, n_patches))
        
        for center in centers:
            row, col = center // grid_size, center % grid_size
            # åˆ›å»º3x3æˆ–2x2çš„æ©ç å—
            block_size = random.randint(1, 2)
            for dr in range(-block_size, block_size + 1):
                for dc in range(-block_size, block_size + 1):
                    new_row, new_col = row + dr, col + dc
                    if 0 <= new_row < grid_size and 0 <= new_col < grid_size:
                        patch_idx = new_row * grid_size + new_col
                        masked_patches.add(patch_idx)
        
        # è°ƒæ•´åˆ°ç›®æ ‡æ©ç æ•°é‡
        masked_patches = list(masked_patches)
        if len(masked_patches) > mask_count:
            masked_patches = random.sample(masked_patches, mask_count)
        elif len(masked_patches) < mask_count:
            # éšæœºæ·»åŠ æ›´å¤šæ©ç 
            remaining = [i for i in range(n_patches) if i not in masked_patches]
            additional = random.sample(
                remaining, 
                min(mask_count - len(masked_patches), len(remaining))
            )
            masked_patches.extend(additional)
        
        # ç”Ÿæˆä¸Šä¸‹æ–‡patchesï¼ˆå¯è§çš„ï¼‰
        context_patches = [i for i in range(n_patches) if i not in masked_patches]
        
        context_patches_list.append(sorted(context_patches))
        # éšæœºé€‰æ‹©ä¸€ä¸ªæ©ç patchä½œä¸ºé¢„æµ‹ç›®æ ‡
        target_patches_list.append(
            random.choice(masked_patches) if masked_patches else 0
        )
    
    return context_patches_list, target_patches_list