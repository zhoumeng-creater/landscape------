"""
å¾®è°ƒæ¨¡å— - åˆ†ç±»å™¨å¾®è°ƒè®­ç»ƒ
"""
import torch
import torch.nn as nn
import torch.optim as optim
import os
from config import Config


def create_finetune_optimizer(ijepa_model, classifier, stage, learning_rate):
    """åˆ›å»ºå¾®è°ƒä¼˜åŒ–å™¨
    
    Args:
        ijepa_model: I-JEPAæ¨¡å‹
        classifier: åˆ†ç±»å™¨
        stage: è®­ç»ƒé˜¶æ®µï¼ˆ1æˆ–2ï¼‰
        learning_rate: åŸºç¡€å­¦ä¹ ç‡
        
    Returns:
        optimizer: ä¼˜åŒ–å™¨
    """
    if stage == 1:
        # é˜¶æ®µ1ï¼šåªè®­ç»ƒåˆ†ç±»å™¨
        param_groups = [
            {'params': classifier.parameters(), 'lr': learning_rate}
        ]
    else:
        # é˜¶æ®µ2ï¼šå¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹çš„æœ€åå‡ å±‚ + åˆ†ç±»å™¨
        param_groups = [
            {
                'params': [p for n, p in ijepa_model.named_parameters() if p.requires_grad], 
                'lr': learning_rate / Config.FINETUNE_LR_RATIO
            },
            {'params': classifier.parameters(), 'lr': learning_rate}
        ]
    
    optimizer = optim.AdamW(param_groups, weight_decay=Config.FINETUNE_WEIGHT_DECAY)
    return optimizer


def train_epoch(ijepa_model, classifier, train_loader, criterion, optimizer, device, epoch):
    """è®­ç»ƒä¸€ä¸ªepoch
    
    Args:
        ijepa_model: I-JEPAæ¨¡å‹
        classifier: åˆ†ç±»å™¨
        train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
        criterion: æŸå¤±å‡½æ•°
        optimizer: ä¼˜åŒ–å™¨
        device: è®¾å¤‡
        epoch: å½“å‰epoch
        
    Returns:
        avg_loss: å¹³å‡æŸå¤±
        accuracy: å‡†ç¡®ç‡
    """
    # æ ¹æ®epochå†³å®šæ˜¯å¦è®­ç»ƒI-JEPAæ¨¡å‹
    if epoch >= Config.FINETUNE_STAGE2_EPOCH:
        ijepa_model.train()
    else:
        ijepa_model.eval()
    
    classifier.train()
    
    total_loss = 0.0
    correct = 0
    total = 0
    
    for batch_idx, (data, target) in enumerate(train_loader):
        data, target = data.to(device), target.to(device)
        
        optimizer.zero_grad()
        
        # æå–ç‰¹å¾
        if epoch >= Config.FINETUNE_STAGE2_EPOCH:
            features = ijepa_model.encode(data)
        else:
            with torch.no_grad():
                features = ijepa_model.encode(data)
        
        # åˆ†ç±»
        output = classifier(features)
        loss = criterion(output, target)
        
        # åå‘ä¼ æ’­
        loss.backward()
        
        # æ¢¯åº¦è£å‰ª
        params_to_clip = list(classifier.parameters())
        if epoch >= Config.FINETUNE_STAGE2_EPOCH:
            params_to_clip.extend([p for p in ijepa_model.parameters() if p.requires_grad])
        torch.nn.utils.clip_grad_norm_(params_to_clip, max_norm=Config.GRADIENT_CLIP)
        
        optimizer.step()
        
        # ç»Ÿè®¡
        total_loss += loss.item()
        _, predicted = torch.max(output.data, 1)
        total += target.size(0)
        correct += (predicted == target).sum().item()
        
        # æ‰“å°è¿›åº¦
        if batch_idx % Config.LOG_INTERVAL == 0:
            print(f'Epoch {epoch+1}, Batch {batch_idx}/{len(train_loader)}, '
                  f'Loss: {loss.item():.4f}, Acc: {100 * correct / total:.2f}%')
    
    avg_loss = total_loss / len(train_loader)
    accuracy = 100 * correct / total
    
    return avg_loss, accuracy


def validate(ijepa_model, classifier, val_loader, criterion, device):
    """éªŒè¯æ¨¡å‹
    
    Args:
        ijepa_model: I-JEPAæ¨¡å‹
        classifier: åˆ†ç±»å™¨
        val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
        criterion: æŸå¤±å‡½æ•°
        device: è®¾å¤‡
        
    Returns:
        avg_loss: å¹³å‡æŸå¤±
        accuracy: å‡†ç¡®ç‡
    """
    ijepa_model.eval()
    classifier.eval()
    
    total_loss = 0.0
    correct = 0
    total = 0
    
    with torch.no_grad():
        for data, target in val_loader:
            data, target = data.to(device), target.to(device)
            
            # æå–ç‰¹å¾
            features = ijepa_model.encode(data)
            
            # åˆ†ç±»
            output = classifier(features)
            loss = criterion(output, target)
            
            # ç»Ÿè®¡
            total_loss += loss.item()
            _, predicted = torch.max(output.data, 1)
            total += target.size(0)
            correct += (predicted == target).sum().item()
    
    avg_loss = total_loss / len(val_loader)
    accuracy = 100 * correct / total
    
    return avg_loss, accuracy


def optimized_finetune_classifier(ijepa_model, classifier, train_loader, val_loader, 
                                 num_epochs=None, learning_rate=None):
    """ä¼˜åŒ–çš„åˆ†ç±»å™¨å¾®è°ƒ
    
    Args:
        ijepa_model: I-JEPAæ¨¡å‹
        classifier: åˆ†ç±»å™¨
        train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
        val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
        num_epochs: è®­ç»ƒè½®æ•°
        learning_rate: å­¦ä¹ ç‡
        
    Returns:
        classifier: è®­ç»ƒåçš„åˆ†ç±»å™¨
        history: è®­ç»ƒå†å²
    """
    # ä½¿ç”¨é»˜è®¤é…ç½®
    if num_epochs is None:
        num_epochs = Config.FINETUNE_EPOCHS
    if learning_rate is None:
        learning_rate = Config.FINETUNE_LR
    
    device = Config.DEVICE
    ijepa_model = ijepa_model.to(device)
    classifier = classifier.to(device)
    
    # æŸå¤±å‡½æ•°ï¼ˆå¸¦æ ‡ç­¾å¹³æ»‘ï¼‰
    criterion = nn.CrossEntropyLoss(label_smoothing=Config.LABEL_SMOOTHING)
    
    # åˆå§‹ä¼˜åŒ–å™¨ï¼ˆé˜¶æ®µ1ï¼‰
    optimizer = create_finetune_optimizer(ijepa_model, classifier, stage=1, learning_rate=learning_rate)
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler = optim.lr_scheduler.OneCycleLR(
        optimizer, max_lr=learning_rate, 
        epochs=num_epochs, steps_per_epoch=len(train_loader),
        pct_start=0.1, anneal_strategy='cos'
    )
    
    # è®­ç»ƒå†å²
    history = {
        'train_loss': [],
        'val_loss': [],
        'train_acc': [],
        'val_acc': []
    }
    
    best_val_acc = 0.0
    patience_counter = 0
    
    print("ğŸš€ å¼€å§‹ä¼˜åŒ–ç‰ˆåˆ†ç±»å™¨å¾®è°ƒ...")
    
    for epoch in range(num_epochs):
        # é˜¶æ®µåˆ‡æ¢
        if epoch == Config.FINETUNE_STAGE2_EPOCH:
            print(f"ğŸ”„ åˆ‡æ¢åˆ°é˜¶æ®µ2ï¼šå¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹çš„æœ€åå‡ å±‚")
            
            # è§£å†»æœ€åå‡ å±‚
            for name, param in ijepa_model.named_parameters():
                if 'blocks.10' in name or 'blocks.11' in name or 'feature_enhancer' in name:
                    param.requires_grad = True
            
            # é‡æ–°åˆ›å»ºä¼˜åŒ–å™¨
            optimizer = create_finetune_optimizer(
                ijepa_model, classifier, stage=2, learning_rate=learning_rate
            )
            
            # é‡æ–°åˆ›å»ºè°ƒåº¦å™¨
            remaining_epochs = num_epochs - Config.FINETUNE_STAGE2_EPOCH
            scheduler = optim.lr_scheduler.OneCycleLR(
                optimizer, 
                max_lr=[learning_rate / Config.FINETUNE_LR_RATIO, learning_rate], 
                epochs=remaining_epochs, 
                steps_per_epoch=len(train_loader)
            )
        
        # è®­ç»ƒ
        train_loss, train_acc = train_epoch(
            ijepa_model, classifier, train_loader, 
            criterion, optimizer, device, epoch
        )
        
        # æ›´æ–°å­¦ä¹ ç‡
        if epoch < Config.FINETUNE_STAGE2_EPOCH or epoch >= Config.FINETUNE_STAGE2_EPOCH:
            scheduler.step()
        
        # éªŒè¯
        val_loss, val_acc = validate(
            ijepa_model, classifier, val_loader, criterion, device
        )
        
        # è®°å½•å†å²
        history['train_loss'].append(train_loss)
        history['val_loss'].append(val_loss)
        history['train_acc'].append(train_acc)
        history['val_acc'].append(val_acc)
        
        # æ‰“å°ç»“æœ
        print(f'ğŸ“Š Epoch {epoch+1}/{num_epochs}:')
        print(f'  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')
        print(f'  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')
        print('-' * 60)
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            patience_counter = 0
            
            save_finetune_checkpoint(
                ijepa_model, classifier, epoch, 
                val_acc, val_loss, train_acc, train_loss
            )
            print(f"âœ… ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ŒéªŒè¯å‡†ç¡®ç‡: {val_acc:.2f}%")
        else:
            patience_counter += 1
        
        # æ—©åœ
        if patience_counter >= Config.PATIENCE_FINETUNE:
            print(f"æ—©åœè§¦å‘ï¼Œåœ¨ç¬¬ {epoch+1} è½®åœæ­¢è®­ç»ƒ")
            break
    
    # åŠ è½½æœ€ä½³æ¨¡å‹
    ijepa_model, classifier = load_finetune_checkpoint(ijepa_model, classifier)
    
    print(f"ğŸ‰ ä¼˜åŒ–ç‰ˆå¾®è°ƒå®Œæˆ! æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.2f}%")
    
    return classifier, history


def save_finetune_checkpoint(ijepa_model, classifier, epoch, val_acc, val_loss, train_acc, train_loss):
    """ä¿å­˜å¾®è°ƒæ£€æŸ¥ç‚¹
    
    Args:
        ijepa_model: I-JEPAæ¨¡å‹
        classifier: åˆ†ç±»å™¨
        epoch: å½“å‰epoch
        val_acc: éªŒè¯å‡†ç¡®ç‡
        val_loss: éªŒè¯æŸå¤±
        train_acc: è®­ç»ƒå‡†ç¡®ç‡
        train_loss: è®­ç»ƒæŸå¤±
    """
    checkpoint = {
        'epoch': epoch + 1,
        'ijepa_state': ijepa_model.state_dict(),
        'classifier_state': classifier.state_dict(),
        'val_accuracy': val_acc,
        'val_loss': val_loss,
        'train_accuracy': train_acc,
        'train_loss': train_loss
    }
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs(Config.CHECKPOINT_DIR, exist_ok=True)
    
    # ä¿å­˜æ£€æŸ¥ç‚¹
    checkpoint_path = os.path.join(Config.CHECKPOINT_DIR, Config.FINETUNE_MODEL_PATH)
    torch.save(checkpoint, checkpoint_path)


def load_finetune_checkpoint(ijepa_model, classifier, checkpoint_path=None):
    """åŠ è½½å¾®è°ƒæ£€æŸ¥ç‚¹
    
    Args:
        ijepa_model: I-JEPAæ¨¡å‹
        classifier: åˆ†ç±»å™¨
        checkpoint_path: æ£€æŸ¥ç‚¹è·¯å¾„
        
    Returns:
        ijepa_model: åŠ è½½äº†æƒé‡çš„I-JEPAæ¨¡å‹
        classifier: åŠ è½½äº†æƒé‡çš„åˆ†ç±»å™¨
    """
    if checkpoint_path is None:
        checkpoint_path = os.path.join(Config.CHECKPOINT_DIR, Config.FINETUNE_MODEL_PATH)
    
    try:
        checkpoint = torch.load(checkpoint_path, map_location=Config.DEVICE)
        ijepa_model.load_state_dict(checkpoint['ijepa_state'])
        classifier.load_state_dict(checkpoint['classifier_state'])
        print(f"âœ… åŠ è½½æœ€ä½³æ¨¡å‹ï¼ŒéªŒè¯å‡†ç¡®ç‡: {checkpoint['val_accuracy']:.2f}%")
        return ijepa_model, classifier
    except Exception as e:
        print(f"âŒ åŠ è½½å¾®è°ƒæ¨¡å‹å¤±è´¥: {e}")
        return ijepa_model, classifier