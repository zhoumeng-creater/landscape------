"""
é«˜çº§è®­ç»ƒæ¨¡å— - æ”¯æŒé¢„è®­ç»ƒæ¨¡å‹ã€SAMã€æ··åˆç²¾åº¦ç­‰
"""
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.cuda.amp import autocast, GradScaler
import numpy as np
import os
import time
from config import Config
from utils.losses import FocalLoss, MixupCrossEntropy


class SAMOptimizer:
    """SAM (Sharpness Aware Minimization) ä¼˜åŒ–å™¨åŒ…è£…å™¨"""
    def __init__(self, base_optimizer, rho=0.05):
        self.base_optimizer = base_optimizer
        self.rho = rho
        
    def first_step(self):
        """è®¡ç®—æ¢¯åº¦å¹¶åœ¨å‚æ•°ç©ºé—´ä¸­ç§»åŠ¨"""
        self.backup = {}
        for group in self.base_optimizer.param_groups:
            for p in group['params']:
                if p.grad is None:
                    continue
                self.backup[p] = p.data.clone()
                grad_norm = p.grad.data.norm(2)
                epsilon = self.rho / (grad_norm + 1e-12)
                p.data.add_(p.grad.data, alpha=epsilon)
    
    def second_step(self):
        """æ¢å¤å‚æ•°å¹¶ä½¿ç”¨SAMæ¢¯åº¦æ›´æ–°"""
        for group in self.base_optimizer.param_groups:
            for p in group['params']:
                if p in self.backup:
                    p.data = self.backup[p]
        self.base_optimizer.step()
        self.base_optimizer.zero_grad()


def create_optimizer(model, learning_rate):
    """åˆ›å»ºä¼˜åŒ–å™¨
    
    Args:
        model: æ¨¡å‹
        learning_rate: å­¦ä¹ ç‡
        
    Returns:
        optimizer: ä¼˜åŒ–å™¨
        use_sam: æ˜¯å¦ä½¿ç”¨SAM
    """
    # åˆ†ç¦»ä¸åŒéƒ¨åˆ†çš„å‚æ•°
    backbone_params = []
    head_params = []
    other_params = []
    
    for name, param in model.named_parameters():
        if not param.requires_grad:
            continue
            
        if 'backbone' in name:
            backbone_params.append(param)
        elif 'classifier' in name or 'aux_classifier' in name:
            head_params.append(param)
        else:
            other_params.append(param)
    
    # ä¸åŒéƒ¨åˆ†ä½¿ç”¨ä¸åŒçš„å­¦ä¹ ç‡
    param_groups = [
        {'params': backbone_params, 'lr': learning_rate * 0.1, 'weight_decay': 0.01},
        {'params': head_params, 'lr': learning_rate, 'weight_decay': 0.001},
        {'params': other_params, 'lr': learning_rate * 0.5, 'weight_decay': 0.01}
    ]
    
    # é€‰æ‹©ä¼˜åŒ–å™¨
    if Config.OPTIMIZER_TYPE == 'adamw':
        base_optimizer = optim.AdamW(param_groups)
    elif Config.OPTIMIZER_TYPE == 'lamb':
        # éœ€è¦å®‰è£…: pip install torch-optimizer
        from torch_optimizer import Lamb
        base_optimizer = Lamb(param_groups)
    else:
        base_optimizer = optim.Adam(param_groups)
    
    # æ˜¯å¦ä½¿ç”¨SAM
    if Config.USE_SAM:
        return SAMOptimizer(base_optimizer, Config.SAM_RHO), True
    else:
        return base_optimizer, False


def mixup_data(x, y, alpha=1.0):
    """Mixupæ•°æ®å¢å¼º"""
    if alpha > 0:
        lam = np.random.beta(alpha, alpha)
    else:
        lam = 1

    batch_size = x.size()[0]
    index = torch.randperm(batch_size).to(x.device)

    mixed_x = lam * x + (1 - lam) * x[index]
    y_a, y_b = y, y[index]
    
    return mixed_x, y_a, y_b, lam


def train_one_epoch(model, train_loader, criterion, optimizer, scaler, epoch, 
                   use_sam=False, accumulation_steps=1):
    """è®­ç»ƒä¸€ä¸ªepoch
    
    Args:
        model: æ¨¡å‹
        train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
        criterion: æŸå¤±å‡½æ•°
        optimizer: ä¼˜åŒ–å™¨
        scaler: æ··åˆç²¾åº¦ç¼©æ”¾å™¨
        epoch: å½“å‰epoch
        use_sam: æ˜¯å¦ä½¿ç”¨SAM
        accumulation_steps: æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
        
    Returns:
        avg_loss: å¹³å‡æŸå¤±
        accuracy: å‡†ç¡®ç‡
    """
    model.train()
    total_loss = 0
    correct = 0
    total = 0
    
    # MixupæŸå¤±å‡½æ•°
    mixup_criterion = MixupCrossEntropy() if Config.USE_MIXUP else None
    
    for batch_idx, (data, target) in enumerate(train_loader):
        data, target = data.to(Config.DEVICE), target.to(Config.DEVICE)
        
        # Mixupæ•°æ®å¢å¼º
        if Config.USE_MIXUP and np.random.random() > 0.5:
            data, target_a, target_b, lam = mixup_data(data, target, Config.MIXUP_ALPHA)
            use_mixup = True
        else:
            use_mixup = False
        
        # æ··åˆç²¾åº¦è®­ç»ƒ
        with autocast(enabled=Config.USE_AMP):
            output = model(data)
            
            # å¤„ç†ä¸åŒçš„è¾“å‡ºæ ¼å¼
            if isinstance(output, tuple):
                logits, aux_logits = output
            else:
                logits, aux_logits = output, None
            
            # è®¡ç®—æŸå¤±
            if use_mixup:
                loss = mixup_criterion(logits, target_a, target_b, lam)
            else:
                loss = criterion(logits, target)
            
            # æ·»åŠ è¾…åŠ©æŸå¤±
            if aux_logits is not None and Config.USE_AUXILIARY_LOSS:
                if use_mixup:
                    aux_loss = mixup_criterion(aux_logits, target_a, target_b, lam)
                else:
                    aux_loss = criterion(aux_logits, target)
                loss = loss + Config.AUX_LOSS_WEIGHT * aux_loss
            
            # æ¢¯åº¦ç´¯ç§¯
            loss = loss / accumulation_steps
        
        # åå‘ä¼ æ’­
        scaler.scale(loss).backward()
        
        # æ¢¯åº¦ç´¯ç§¯æ­¥éª¤
        if (batch_idx + 1) % accumulation_steps == 0:
            # æ¢¯åº¦è£å‰ª
            scaler.unscale_(optimizer if not use_sam else optimizer.base_optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), Config.GRADIENT_CLIP)
            
            if use_sam:
                # SAMç¬¬ä¸€æ­¥
                scaler.step(optimizer.base_optimizer)
                scaler.update()
                optimizer.first_step()
                
                # SAMç¬¬äºŒæ­¥ï¼šé‡æ–°è®¡ç®—æ¢¯åº¦
                optimizer.base_optimizer.zero_grad()
                with autocast(enabled=Config.USE_AMP):
                    output2 = model(data)
                    if isinstance(output2, tuple):
                        logits2, aux_logits2 = output2
                    else:
                        logits2 = output2
                    
                    if use_mixup:
                        loss2 = mixup_criterion(logits2, target_a, target_b, lam)
                    else:
                        loss2 = criterion(logits2, target)
                    loss2 = loss2 / accumulation_steps
                
                scaler.scale(loss2).backward()
                scaler.unscale_(optimizer.base_optimizer)
                torch.nn.utils.clip_grad_norm_(model.parameters(), Config.GRADIENT_CLIP)
                optimizer.second_step()
            else:
                # æ™®é€šä¼˜åŒ–å™¨æ­¥éª¤
                scaler.step(optimizer)
                scaler.update()
                optimizer.zero_grad()
        
        # ç»Ÿè®¡
        total_loss += loss.item() * accumulation_steps
        if not use_mixup:
            _, predicted = torch.max(logits, 1)
            total += target.size(0)
            correct += (predicted == target).sum().item()
        
        # æ‰“å°è¿›åº¦
        if batch_idx % Config.LOG_INTERVAL == 0:
            if use_mixup:
                print(f'Epoch {epoch}, Batch {batch_idx}/{len(train_loader)}, '
                      f'Loss: {loss.item()*accumulation_steps:.4f} (Mixup)')
            else:
                acc = 100 * correct / total if total > 0 else 0
                print(f'Epoch {epoch}, Batch {batch_idx}/{len(train_loader)}, '
                      f'Loss: {loss.item()*accumulation_steps:.4f}, Acc: {acc:.2f}%')
    
    avg_loss = total_loss / len(train_loader)
    accuracy = 100 * correct / total if total > 0 else 0
    
    return avg_loss, accuracy


def validate(model, val_loader, criterion):
    """éªŒè¯æ¨¡å‹"""
    model.eval()
    total_loss = 0
    correct = 0
    total = 0
    
    with torch.no_grad():
        for data, target in val_loader:
            data, target = data.to(Config.DEVICE), target.to(Config.DEVICE)
            
            with autocast(enabled=Config.USE_AMP):
                output = model(data)
                
                if isinstance(output, tuple):
                    logits, _ = output
                else:
                    logits = output
                
                loss = criterion(logits, target)
            
            total_loss += loss.item()
            _, predicted = torch.max(logits, 1)
            total += target.size(0)
            correct += (predicted == target).sum().item()
    
    avg_loss = total_loss / len(val_loader)
    accuracy = 100 * correct / total
    
    return avg_loss, accuracy


def train_pretrained_model(model, train_loader, val_loader, num_epochs, num_classes):
    """è®­ç»ƒé¢„è®­ç»ƒæ¨¡å‹
    
    Args:
        model: é¢„è®­ç»ƒæ¨¡å‹
        train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
        val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
        num_epochs: è®­ç»ƒè½®æ•°
        num_classes: ç±»åˆ«æ•°
        
    Returns:
        model: è®­ç»ƒåçš„æ¨¡å‹
        history: è®­ç»ƒå†å²
    """
    device = Config.DEVICE
    model = model.to(device)
    
    # æŸå¤±å‡½æ•°
    if Config.LABEL_SMOOTHING > 0:
        criterion = nn.CrossEntropyLoss(label_smoothing=Config.LABEL_SMOOTHING)
    else:
        criterion = nn.CrossEntropyLoss()
    
    # ä¼˜åŒ–å™¨
    optimizer, use_sam = create_optimizer(model, Config.FINETUNE_LR)
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler = optim.lr_scheduler.OneCycleLR(
        optimizer if not use_sam else optimizer.base_optimizer,
        max_lr=Config.FINETUNE_LR,
        epochs=num_epochs,
        steps_per_epoch=len(train_loader) // Config.GRADIENT_ACCUMULATION_STEPS,
        pct_start=0.1,
        anneal_strategy='cos'
    )
    
    # æ··åˆç²¾åº¦è®­ç»ƒ
    scaler = GradScaler(enabled=Config.USE_AMP)
    
    # è®­ç»ƒå†å²
    history = {
        'train_loss': [],
        'val_loss': [],
        'train_acc': [],
        'val_acc': []
    }
    
    best_val_acc = 0
    patience_counter = 0
    start_time = time.time()
    
    print("ğŸš€ å¼€å§‹è®­ç»ƒé¢„è®­ç»ƒæ¨¡å‹...")
    print(f"  ä½¿ç”¨SAM: {use_sam}")
    print(f"  ä½¿ç”¨æ··åˆç²¾åº¦: {Config.USE_AMP}")
    print(f"  æ¢¯åº¦ç´¯ç§¯æ­¥æ•°: {Config.GRADIENT_ACCUMULATION_STEPS}")
    
    for epoch in range(num_epochs):
        # åŠ¨æ€è§£å†»å±‚ï¼ˆæ¸è¿›å¼è§£å†»ï¼‰
        if epoch == Config.FINETUNE_STAGE2_EPOCH:
            print(f"\nğŸ”„ Epoch {epoch}: è§£å†»æ›´å¤šå±‚")
            unfreeze_layers(model, stage=2)
        
        # è®­ç»ƒ
        train_loss, train_acc = train_one_epoch(
            model, train_loader, criterion, optimizer, scaler, 
            epoch, use_sam, Config.GRADIENT_ACCUMULATION_STEPS
        )
        
        # æ›´æ–°å­¦ä¹ ç‡
        scheduler.step()
        
        # éªŒè¯
        val_loss, val_acc = validate(model, val_loader, criterion)
        
        # è®°å½•å†å²
        history['train_loss'].append(train_loss)
        history['val_loss'].append(val_loss)
        history['train_acc'].append(train_acc)
        history['val_acc'].append(val_acc)
        
        # æ‰“å°ç»“æœ
        current_lr = optimizer.param_groups[0]['lr'] if not use_sam else optimizer.base_optimizer.param_groups[0]['lr']
        print(f'\nğŸ“Š Epoch {epoch+1}/{num_epochs}:')
        print(f'  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')
        print(f'  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')
        print(f'  Learning Rate: {current_lr:.2e}')
        print('-' * 60)
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            patience_counter = 0
            save_checkpoint(model, optimizer, epoch, val_acc, val_loss)
            print(f"âœ… ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ŒéªŒè¯å‡†ç¡®ç‡: {val_acc:.2f}%")
        else:
            patience_counter += 1
        
        # æ—©åœ
        if patience_counter >= Config.PATIENCE_FINETUNE:
            print(f"æ—©åœè§¦å‘ï¼Œåœ¨ç¬¬ {epoch+1} è½®åœæ­¢è®­ç»ƒ")
            break
    
    # åŠ è½½æœ€ä½³æ¨¡å‹
    load_checkpoint(model)
    
    total_time = time.time() - start_time
    print(f"\nğŸ‰ è®­ç»ƒå®Œæˆ! æ€»è€—æ—¶: {total_time/60:.1f}åˆ†é’Ÿ")
    print(f"æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.2f}%")
    
    return model, history


def unfreeze_layers(model, stage=2):
    """æ¸è¿›å¼è§£å†»å±‚"""
    if hasattr(model, 'backbone'):
        if hasattr(model.backbone, 'blocks'):  # Vision Transformer
            total_blocks = len(model.backbone.blocks)
            if stage == 2:
                # è§£å†»ååŠéƒ¨åˆ†
                for i in range(total_blocks // 2, total_blocks):
                    for param in model.backbone.blocks[i].parameters():
                        param.requires_grad = True
            elif stage == 3:
                # è§£å†»æ‰€æœ‰å±‚
                for param in model.backbone.parameters():
                    param.requires_grad = True


def save_checkpoint(model, optimizer, epoch, val_acc, val_loss):
    """ä¿å­˜æ£€æŸ¥ç‚¹"""
    checkpoint = {
        'epoch': epoch + 1,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict() if hasattr(optimizer, 'state_dict') else optimizer.base_optimizer.state_dict(),
        'val_accuracy': val_acc,
        'val_loss': val_loss,
    }
    
    checkpoint_path = os.path.join(Config.CHECKPOINT_DIR, Config.PRETRAINED_MODEL_PATH)
    torch.save(checkpoint, checkpoint_path)


def load_checkpoint(model, checkpoint_path=None):
    """åŠ è½½æ£€æŸ¥ç‚¹"""
    if checkpoint_path is None:
        checkpoint_path = os.path.join(Config.CHECKPOINT_DIR, Config.PRETRAINED_MODEL_PATH)
    
    if os.path.exists(checkpoint_path):
        checkpoint = torch.load(checkpoint_path, map_location=Config.DEVICE)
        model.load_state_dict(checkpoint['model_state_dict'])
        print(f"âœ… åŠ è½½æ¨¡å‹æ£€æŸ¥ç‚¹ï¼ŒéªŒè¯å‡†ç¡®ç‡: {checkpoint['val_accuracy']:.2f}%")
    
    return model