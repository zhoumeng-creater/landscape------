"""
è¯„ä¼°å‡½æ•°æ¨¡å— - æ¨¡å‹è¯„ä¼°ç›¸å…³å‡½æ•°
"""
import torch
import torch.nn.functional as F
import numpy as np
from sklearn.metrics import (
    accuracy_score, f1_score, precision_score, recall_score,
    classification_report, confusion_matrix, roc_auc_score,
    balanced_accuracy_score, cohen_kappa_score
)
from config import Config


def evaluate_optimized_model(ijepa_model, classifier, test_loader, label_encoder):
    """è¯„ä¼°ä¼˜åŒ–æ¨¡å‹
    
    Args:
        ijepa_model: I-JEPAæ¨¡å‹
        classifier: åˆ†ç±»å™¨
        test_loader: æµ‹è¯•æ•°æ®åŠ è½½å™¨
        label_encoder: æ ‡ç­¾ç¼–ç å™¨
        
    Returns:
        results: è¯„ä¼°ç»“æœå­—å…¸
        predictions: æ‰€æœ‰é¢„æµ‹
        targets: æ‰€æœ‰çœŸå®æ ‡ç­¾
    """
    device = Config.DEVICE
    ijepa_model.eval()
    classifier.eval()
    
    all_predictions = []
    all_targets = []
    all_probabilities = []
    
    print("ğŸ” æ­£åœ¨è¯„ä¼°ä¼˜åŒ–æ¨¡å‹...")
    
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            
            # æå–ç‰¹å¾
            features = ijepa_model.encode(data)
            
            # åˆ†ç±»é¢„æµ‹
            output = classifier(features)
            probabilities = F.softmax(output, dim=1)
            _, predicted = torch.max(output, 1)
            
            all_predictions.extend(predicted.cpu().numpy())
            all_targets.extend(target.cpu().numpy())
            all_probabilities.extend(probabilities.cpu().numpy())
    
    # è½¬æ¢ä¸ºnumpyæ•°ç»„
    all_predictions = np.array(all_predictions)
    all_targets = np.array(all_targets)
    all_probabilities = np.array(all_probabilities)
    
    # è®¡ç®—å„ç§æŒ‡æ ‡
    results = calculate_metrics(all_targets, all_predictions, all_probabilities)
    
    # æ‰“å°ç»“æœ
    print_evaluation_results(results, label_encoder)
    
    # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
    # generate_classification_report(all_targets, all_predictions, label_encoder)
    
    return results, all_predictions, all_targets


def calculate_metrics(y_true, y_pred, y_prob=None):
    """è®¡ç®—å„ç§è¯„ä¼°æŒ‡æ ‡
    
    Args:
        y_true: çœŸå®æ ‡ç­¾
        y_pred: é¢„æµ‹æ ‡ç­¾
        y_prob: é¢„æµ‹æ¦‚ç‡ï¼ˆå¯é€‰ï¼‰
        
    Returns:
        metrics: æŒ‡æ ‡å­—å…¸
    """
    metrics = {}
    
    # åŸºç¡€æŒ‡æ ‡
    metrics['accuracy'] = accuracy_score(y_true, y_pred)
    metrics['balanced_accuracy'] = balanced_accuracy_score(y_true, y_pred)
    metrics['cohen_kappa'] = cohen_kappa_score(y_true, y_pred)
    
    # F1åˆ†æ•°ï¼ˆå¤šç§å¹³å‡æ–¹å¼ï¼‰
    metrics['f1_macro'] = f1_score(y_true, y_pred, average='macro')
    metrics['f1_micro'] = f1_score(y_true, y_pred, average='micro')
    metrics['f1_weighted'] = f1_score(y_true, y_pred, average='weighted')
    
    # ç²¾ç¡®ç‡å’Œå¬å›ç‡
    metrics['precision_macro'] = precision_score(y_true, y_pred, average='macro')
    metrics['precision_weighted'] = precision_score(y_true, y_pred, average='weighted')
    metrics['recall_macro'] = recall_score(y_true, y_pred, average='macro')
    metrics['recall_weighted'] = recall_score(y_true, y_pred, average='weighted')
    
    # å¦‚æœæœ‰æ¦‚ç‡é¢„æµ‹ï¼Œè®¡ç®—AUC
    if y_prob is not None:
        try:
            # å¤šåˆ†ç±»AUC
            num_classes = y_prob.shape[1]
            if num_classes > 2:
                metrics['auc_macro'] = roc_auc_score(y_true, y_prob, 
                                                    multi_class='ovr', average='macro')
                metrics['auc_weighted'] = roc_auc_score(y_true, y_prob, 
                                                       multi_class='ovr', average='weighted')
        except:
            metrics['auc_macro'] = None
            metrics['auc_weighted'] = None
    
    # æ¯ä¸ªç±»åˆ«çš„æŒ‡æ ‡
    metrics['per_class_f1'] = f1_score(y_true, y_pred, average=None)
    metrics['per_class_precision'] = precision_score(y_true, y_pred, average=None)
    metrics['per_class_recall'] = recall_score(y_true, y_pred, average=None)
    
    return metrics


def print_evaluation_results(results, label_encoder):
    """æ‰“å°è¯„ä¼°ç»“æœ
    
    Args:
        results: è¯„ä¼°ç»“æœå­—å…¸
        label_encoder: æ ‡ç­¾ç¼–ç å™¨
    """
    print("\n" + "="*60)
    print("ğŸ“Š ä¼˜åŒ–æ¨¡å‹æµ‹è¯•ç»“æœ:")
    print("="*60)
    
    # æ‰“å°æ€»ä½“æŒ‡æ ‡
    print(f"\nğŸ¯ æ€»ä½“æ€§èƒ½æŒ‡æ ‡:")
    print(f"  å‡†ç¡®ç‡ (Accuracy): {results['accuracy']:.4f} ({results['accuracy']*100:.2f}%)")
    print(f"  å¹³è¡¡å‡†ç¡®ç‡ (Balanced Accuracy): {results['balanced_accuracy']:.4f}")
    print(f"  Cohen's Kappa: {results['cohen_kappa']:.4f}")
    
    print(f"\nğŸ“ˆ F1åˆ†æ•°:")
    print(f"  å®å¹³å‡ (Macro): {results['f1_macro']:.4f}")
    print(f"  å¾®å¹³å‡ (Micro): {results['f1_micro']:.4f}")
    print(f"  åŠ æƒå¹³å‡ (Weighted): {results['f1_weighted']:.4f}")
    
    print(f"\nğŸª ç²¾ç¡®ç‡å’Œå¬å›ç‡:")
    print(f"  ç²¾ç¡®ç‡ - å®å¹³å‡: {results['precision_macro']:.4f}")
    print(f"  ç²¾ç¡®ç‡ - åŠ æƒå¹³å‡: {results['precision_weighted']:.4f}")
    print(f"  å¬å›ç‡ - å®å¹³å‡: {results['recall_macro']:.4f}")
    print(f"  å¬å›ç‡ - åŠ æƒå¹³å‡: {results['recall_weighted']:.4f}")
    
    if results.get('auc_macro') is not None:
        print(f"\nğŸ“Š AUCåˆ†æ•°:")
        print(f"  å®å¹³å‡: {results['auc_macro']:.4f}")
        print(f"  åŠ æƒå¹³å‡: {results['auc_weighted']:.4f}")
    
    # æ‰“å°æ¯ä¸ªç±»åˆ«çš„æ€§èƒ½
    print(f"\nğŸ·ï¸ æ¯ä¸ªç±»åˆ«çš„æ€§èƒ½:")
    class_names = label_encoder.classes_
    for i, class_name in enumerate(class_names):
        if i < len(results['per_class_f1']):
            print(f"  {class_name}:")
            print(f"    F1: {results['per_class_f1'][i]:.3f} | "
                  f"ç²¾ç¡®ç‡: {results['per_class_precision'][i]:.3f} | "
                  f"å¬å›ç‡: {results['per_class_recall'][i]:.3f}")
    
    print("="*60)